---
title: "Exam 1 BMI 6106"
output:
  html_document:
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: no
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: '2'
  word_document:
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Due March 25th 11:59pm

### As for other problem sets for this course, all answers should be submitted as a single .R file. To receive full credit for answers, make sure to (1) annotate your code, (2) use annotations to explain your reasoning, and (3) ensure that your code can be run. 

#### In this assignment we are going to review the concepts from class this week. However, concepts build on each other for every test you run you need to make sure that the data meets the appropiate criteria to be able to run the test ####Important: Use comments throughout your code to:

#### Indicate which question you are answering

#### Example: #Question 1A, #Question 1B, etc

#### Annotate your work

#### Examples: #Calculate the mean for ‘height’

#### P(being on time) = 1/6

#### If you do not provide annotation, we will not be able to give you full credit for solving the problems.

#### If you have any questions about what is being asked or what you need to do in order to solve the problems, please reach out to a TA or instructor as soon as possible. If it is necessary to provide additional information or corrections, updated information will be posted on the HW3 Assignment page in Canvas.


## Q1. 5 points 

### Which of the following statements about multicollinearity in regression analysis is true?

#### a. Multicollinearity occurs when the correlation between two independent variables is close to 0.

#### b. Multicollinearity can lead to unstable coefficient estimates and inflated standard errors.

#### c. Multicollinearity affects only the interpretation of the intercept term.

#### d. Multicollinearity is more problematic in simple linear regression than in multiple regression.
```{r Q1}
# b. Multicollinearity can lead to unstable coefficient estimates and inflated standard errors.

```
## Q2. 5 points 

### Suppose you are building a regression model to predict housing prices. Which type of variable is “neighborhood”?

#### a. Continuous

#### b. Ordinal

#### c. Nominal

#### d. Interval

#### e. Mixed
```{r Q2}
# c. Nominal

```
## Q3. 5 points 

### In a logistic regression model, what does the odds ratio represent?

#### a. The change in the dependent variable for a one-unit change in the independent variable.

#### b. The probability of the dependent variable being 1.

#### c. The ratio of the odds of success to the odds of failure.

#### d. The coefficient of determination (R-squared).

#### e. The absolute probability derived from the least squares function
```{r Q3}
# c. The ratio of the odds of success to the odds of failure.

```
## Q4. 15 points

### Using the bayes theorem we can derive the (maybe surprising) result that in families that have 2 children along with the knowledge that at least one of the children is a boy born on a Wednesday, the probability that the family has 2 boys is 13/27 (~.48). We can test this calculation by randomly generating many “two child families” and counting the number that fulfill the criteria of (1) having a boy born on a Wednesday and (2) of these families having two boys. 

###  Write R code to simulate 10,000 two-child families and count the frequency of two-boy families amongst all families with at least one Wednesday-born boy. In your answer, you should (A, 8 pts) provide the annotated code and report the frequency your code produced; (B, 4 pts) explain your approach. The code should be annotated and runnable by us if we copy into RStudio. As before, assume that children are born with exactly 50/50 probability that they are a “boy” or a “girl” and that births occur on each day of the week with an equal probability. 




```{r Q4}
#(there are many ways to approach this question). You can create a function that simulates the families or just sample the families n number of times

### Part A:
# set seed for reproducibility
set.seed(1)

# Initialize variables
n_simulations <- 10000
two_boy <- 0
one_wednesday_boy <- 0

# Define possible outcomes for gender and days of the week
genders <- c("Boy", "Girl")
days_of_week <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")

# Simulate 10,000 families
for (i in 1:n_simulations) {
  # Simulate genders for two children
  child_genders <- sample(genders, size = 2, replace = TRUE, prob = c(0.5, 0.5))
  
  # Simulate birth days for two children
  birth_days <- sample(days_of_week, size = 2, replace = TRUE)
  
  # Check if at least one boy was born on a Wednesday
  if ("Wednesday" %in% birth_days[child_genders == "Boy"]) {
    one_wednesday_boy <- one_wednesday_boy + 1
    
    # If there is at least one Wednesday boy, check if both are boys
    if (all(child_genders == "Boy")) {
      two_boy <- two_boy + 1
    }
  }
}
# calculate frequency of two-boy families among those that have at least one boy on Wednesday
frequency <- two_boy / one_wednesday_boy
cat("Frequency: ", frequency )

### Part B:
# In this approach, I generated 2 child genders with 50/50 probability and a birth day with equal probabilities. Then I checked if one of the children was a boy born on a Wednesday, and if true, I added +1 to the one_wednesday_boy count. Then if there was one boy born on Wednesday I checked if both children were boys and if true I added +1 to the two_boy count. I ran this simulation 10000 times and calculated the frequency of two-boy families among those that have at least one boy on Wednesday. The resulting frequency was 0.48, which is what we expected to see.
```

## Q5. 20 points

### Using the attached set of “datasaurus12” datasets 

#### A. produce a table with the following parameters for each dataset (x-mean, y-mean, x-standard deviation, y-standard deviation, Pearson correlation) (5 points)
#### B. Produce a scatteplot for each one of these datasets (ideally in a facetted plot e.g. facet_grap function from ggplot) (5 points)
#### C. Why are these statitical paramenter not sufficient to distinguish any of the datasets from each other. (5 points)
#### D. Identify and use another summary statistic in R that can discriminate between all the datasaurus12 datasets to at least 1 decimal place. You may use any available R package to run your analysis. In your answer, you should provide the name of the function you use, identify the library in which it can be found (if it is not a core R function), and provide a table of the values you obtain from the datasaurus12 datasets. (5 points)



```{r Q5}
library(readr)
library(dplyr)
library(purrr)

# List all CSV files in the folder
csv_files <- list.files(path = "datasaurus12", pattern = "\\.csv$", full.names = TRUE)

# Create a named list where each name is the file name without the extension
names_list <- tools::file_path_sans_ext(basename(csv_files))

# Read each CSV file into its own data frame within a named list
data_frames_list <- setNames(lapply(csv_files, function(file) read_csv(file, show_col_types = FALSE)), names_list)

### Part A:
# Function to calculate summary statistics for each data frame
calculate_summary <- function(df) {
  summary <- df %>%
    summarize(
      x_mean = mean(x, na.rm = TRUE),
      y_mean = mean(y, na.rm = TRUE),
      x_sd = sd(x, na.rm = TRUE),
      y_sd = sd(y, na.rm = TRUE),
      pearson_correlation = cor(x, y, use = "complete.obs")
    )
  return(summary)
}

# Apply the function to each data frame in the list and combine the results
summary_table <- map_df(data_frames_list, calculate_summary, .id = "Dataset")

# Print the summary table
print(summary_table)

### Part B:
# Combine data into single dataset with id as dataset name
combined_data <- bind_rows(data_frames_list, .id = "dataset")

# Plot each dataset in faceted plot
ggplot(combined_data, aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~ dataset, scales = "free") +
  theme_minimal() +
  labs(title = "Datasets Scatterplots", x = "X", y = "Y")

### Part C:
# Statistical parameters like mean, standard deviation, and Pearson correlation might not be sufficient to distinguish datasets from each other because they summarize data in ways that can obscure underlying patterns or anomalies. Different datasets can have identical summary statistics but show vastly different trends when visualized.  

### Part D:
library(e1071)
# Calculate skewness and kurtosis for each dataset
stats_summary <- imap_dfr(data_frames_list, function(df, name) {
  tibble(
    Dataset = name,
    Skewness_X = skewness(df$x, na.rm = TRUE),
    Skewness_Y = skewness(df$y, na.rm = TRUE),

  )
})

# Print the summary statistics
print(stats_summary)

# I found the skewness function, which is part of the e1071 library, to be useful for discriminating these 12 datasets. The skewness for these datasets varies between each set which allows us to distinguish between the sets.

```

------------------------------------------------------------------------

## Q6. 10 points

### The file anscombe.txt contains 4 different datasets constructed by Anscombe in 1973. The x and y variables are matched to each dataset (e.g., for dataset 1, x1 corresponds to y1).

### A. Use R to create a scatterplot for each dataset. In your answer, you should provide the code and clearly annotate which section of code will produce each plot. (2 points)
### B. Use R to run a simple linear regression for each dataset. In your answer, you should provide both the annotated code and the resulting linear function for each model.(2 points)
### C. Use R to generate residuals plots for each dataset. In your answer, you should provide the annotated code. (3 points)
### D. State the assumptions for linear regression. (3 points)
### E. For each model, use a few sentences to (i) describe the model, (ii) indicate if there is a problem with the model, and (iii) explain what can be done to fix it. (5 points)
```{r Q6}
### Part A:
library(ggplot2)

# read txt file
anscombe_data <- read.table("anscombe.txt", header = TRUE, sep = "")

# Names of the x and y columns
x_names <- c("x1", "x2", "x3", "x4")
y_names <- c("y1", "y2", "y3", "y4")

par(mfrow = c(2, 2))  # 2x2 layout for 4 plots

# Loop through each pair of x and y columns
for (i in 1:4) {
  # Create plot
  plot(anscombe_data[[x_names[i]]], anscombe_data[[y_names[i]]], 
       main = paste("Scatterplot of", x_names[i], "vs", y_names[i]),
       xlab = "X Values", ylab = "Y Values", pch = 19)
  
    # Create linear model
  lm_model <- lm(anscombe_data[[y_names[i]]] ~ anscombe_data[[x_names[i]]], data = anscombe_data)
  
  # Add the regression line
  abline(lm_model, col = "blue")
}

### Part B:
# Loop through each pair of x and y columns
for (i in 1:4) {
  # Create linear model
  lm_model <- lm(anscombe_data[[y_names[i]]] ~ anscombe_data[[x_names[i]]], data = anscombe_data)
  
  # Extract coefficients
  intercept <- coef(lm_model)[1]
  slope <- coef(lm_model)[2]
  
  # Print linear function
  cat("Linear function for", x_names[i], "vs", y_names[i], ":\n")
  cat(paste("y =", round(slope, 3), "*", x_names[i], "+", round(intercept, 3),"\n\n"))
}

### Part C:
# Set up the plotting layout
par(mfrow = c(2, 2))  # 2x2 layout for 4 plots

# Loop through each pair of x and y columns
for (i in 1:4) {
  # Create linear model
  lm_model <- lm(anscombe_data[[y_names[i]]] ~ anscombe_data[[x_names[i]]], data = anscombe_data)
  
  # Generate residuals
  residuals <- residuals(lm_model)
  
  # Generate fitted values
  fitted_values <- fitted(lm_model)
  
  # Create residual plot
  plot(fitted_values, residuals, main = paste("Residual Plot for", x_names[i], "vs", y_names[i]),
       xlab = "Fitted Values", ylab = "Residuals", pch = 19)
  
  # Add a horizontal line at y = 0 for reference
  abline(h = 0, col = "red")
}

### Part D:
# Linear regression assumes that there is a linear relationship between the independent variables and the dependent variable, the observations are independent of each other, the variance of the errors is constant, the residuals are normally distributed, there is no perfect multicollinearity among the independent variables, no influential outliers exist, and there is no autocorrelation among the residuals.

### Part E:
# Model x1y1:
# TThis model appears to be a linear relationship with residuals constant around identity line. There doesn't appear to be anything wrong with this model.

# Model x2y2:
# This  plots with a logarithmic-like curve. This model assumes a linear relationship but it is actually curvilinear. Using a curvilinear regrission model would be better for this data.

# Model x3y3:
# This model has data plotted on what appears to a positive-sloped line with a single outlier. It appears that the outlier slightly affects the slope. You could exclude this outlier to get a better model.

# Model x4y4:
# This model  has data plotted on a vertical line with a single outlier. The outlier has a significant influence on the slope. Exclusion of this outlier would lead to a line that is parallel to the y-axis.

```

## Q7. 10 points

### we classify 2000 email in two groups: 1000 emails as spam and 1000 emails as non-spam. 210 of the spam emails contained the phrase This isn’t spam, 99 had the word prize and 110 the word prince. Of the 99 that contained the word prize, 79 also contained the word prince. On the other hand, of the 1000 non-spam emails, only 23 had the phrase this isn’t spam, 80 the word prize and 110 the word prince. Of the 80 that contained the word prize 8 also contained the word prince.


### Assuming that the a priori probability of any message being spam is 0.5, what is the probability that an email is spam given it contains the phrase This isn't spam
```{r Q7}
# Calculate probabilities
p_phrase_given_spam <- 210 / 1000  # Probability of phrase given spam
p_spam <- 0.5  # Prior probability of spam
p_phrase_given_non_spam <- 23 / 1000  # Probability of phrase given non-spam

# Calculate denominator for Bayes' theorem
p_phrase <- (p_phrase_given_spam * p_spam) + (p_phrase_given_non_spam * (1 - p_spam))

# Apply Bayes' theorem
p_spam_given_phrase <- (p_phrase_given_spam * p_spam) / p_phrase

# Print the result
cat("Probability: ",p_spam_given_phrase)

```
## Q8. 15 points

### From the Cleveland Heart Disease dataset (attached to this folder - [ https://archive.ics.uci.edu/ml/datasets/Heart+Disease ] ) 

### A. if we pick 25 patients randomly, find the probability that at least 3 have less than 50 percent diameter narrowing (num variable 14 in dataset state 0 - condition). Answer this question using one of the distributions seen in class you can confirm the answer with the R functions. (7 points)

### B. Assume that patients are examined at random, what is the average number of patients before the first patient with cholesterol levels above than 300 (including 300) is encountered? Answer this question using one of the distributions seen in class you can confirm the answer with the R functions. (8 points)
```{r Q8}
cleveland <- read.csv("heart_cleveland_upload.csv")

### Part A:
# Calculate the number of patients with less than 50 percent diameter narrowing
patients_less_than_50 <- sum(cleveland$condition == 0)

# Total number of patients
total_patients <- nrow(cleveland)

# Calculate the probability of a patient having less than 50 percent diameter narrowing
probability_less_than_50 <- patients_less_than_50 / total_patients

# Calculate the probability of at least 3 out of 25 patients having less than 50% diameter narrowing
probability_at_least_3 <- 1 - pbinom(2, size = 25, prob = probability_less_than_50)

cat("Probability: ", probability_at_least_3, "\n")

### Part B:
# Calculate the probability of a patient having cholesterol levels >= 300
patients_with_cholesterol_above_300 <- sum(cleveland$chol >= 300)
probability_cholesterol_above_300 <- patients_with_cholesterol_above_300 / total_patients

# The mean (or expected value) for the geometric distribution
# This is the average number of patients examined before finding one with cholesterol >= 300
average_patients_before_success <- 1 / probability_cholesterol_above_300

# Print the result
cat("Average number of patients before the first patient with cholesterol levels above than 300: ", average_patients_before_success, "\n")
```
## Q9. 15 points

### From the Cleveland Heart Disease dataset:


### A. calculate the mean and variance for the maximum heart rate achieved during the exercise test (thalach). (2 points)
### B. Generate 10000 bootstraps for this variable (sample with replacement) and calculate the mean, median, and variance for each of the bootstrap samples.Just creating the object(s) that contains these values is sufficient. (3 points)
### C. Calculate the mean, standard deviation, standard error, and 95% CI of the distribution of **means** from the bootstrap samples (one value for each parameter). (3 points)
### D. Produce a density plot of the distribution mean from the bootstraps. (3 points)
### E. In one or two sentences write an overall conclusion of applying the bootstrap to increase the precision to the estimation of this variable. (4 points)
```{r Q9}
### Part A:
# Calculate the max heart rate mean
mean_thalach <- mean(cleveland$thalach)

# Calculate the max heart rate variance
variance_thalach <- var(cleveland$thalach)

### Part B:
thalach <- cleveland$thalach

# Number of bootstrap samples
n_bootstraps <- 10000

# Initialize vectors to store the results
bootstrap_means <- numeric(n_bootstraps)
bootstrap_medians <- numeric(n_bootstraps)
bootstrap_variances <- numeric(n_bootstraps)

# Perform the bootstrap resampling
set.seed(1) # for reproducibility
for (i in 1:n_bootstraps) {
  bootstrap_sample <- sample(thalach, replace = TRUE, size = length(thalach))
  bootstrap_means[i] <- mean(bootstrap_sample)
  bootstrap_medians[i] <- median(bootstrap_sample)
  bootstrap_variances[i] <- var(bootstrap_sample)
}

### Part C:
# Calculate the mean of the bootstrap means
mean_of_means <- mean(bootstrap_means)

# Calculate the standard deviation of the bootstrap means
sd_of_means <- sd(bootstrap_means)

# Calculate the standard error of the mean
se_of_means <- sd_of_means / sqrt(length(bootstrap_means))

# Calculate the 95% confidence interval of the mean
ci_95 <- quantile(bootstrap_means, c(0.025, 0.975))

# Print the results
cat("Mean of bootstrap means:", mean_of_means, "\n")
cat("Standard deviation of bootstrap means:", sd_of_means, "\n")
cat("Standard error of bootstrap means:", se_of_means, "\n")
cat("95% CI of bootstrap means:", ci_95[1], "-", ci_95[2], "\n")

### Part D:

# Create a density plot for the bootstrap means
ggplot(data.frame(BootstrapMeans = bootstrap_means), aes(x = BootstrapMeans)) +
  geom_density(fill="blue", alpha=0.5) +
  labs(title="Density Plot of Bootstrap Means",
       x="Mean of Bootstrap Samples",
       y="Density") +
  theme_minimal()

### Part E: 
# Applying bootstrap techniques significantly enhances the precision of estimating the maximum heart rate achieved during exercise by allowing for the computation of a more accurate mean, standard deviation, and confidence interval. This method effectively quantifies the uncertainty and variability of the estimate, providing a more reliable foundation for statistical inference and decision-making related to heart health assessments.

```
